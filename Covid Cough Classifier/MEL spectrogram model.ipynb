{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afedf3f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-13T22:00:43.281936Z",
     "iopub.status.busy": "2023-06-13T22:00:43.281228Z",
     "iopub.status.idle": "2023-06-13T22:00:47.418848Z",
     "shell.execute_reply": "2023-06-13T22:00:47.417912Z"
    },
    "papermill": {
     "duration": 4.1473,
     "end_time": "2023-06-13T22:00:47.421482",
     "exception": false,
     "start_time": "2023-06-13T22:00:43.274182",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "from functools import lru_cache\n",
    "import os\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.io.wavfile\n",
    "import scipy.fftpack\n",
    "import scipy.linalg\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e9eeed0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-13T22:00:47.435110Z",
     "iopub.status.busy": "2023-06-13T22:00:47.433302Z",
     "iopub.status.idle": "2023-06-13T22:00:47.451714Z",
     "shell.execute_reply": "2023-06-13T22:00:47.450908Z"
    },
    "id": "MpjhKDYreIQ8",
    "papermill": {
     "duration": 0.026955,
     "end_time": "2023-06-13T22:00:47.453835",
     "exception": false,
     "start_time": "2023-06-13T22:00:47.426880",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@lru_cache(maxsize=10)\n",
    "def get_window(n, type='hamming'):\n",
    "    coefs = np.arange(n)\n",
    "    window = 0.54 - 0.46 * np.cos(2 * np.pi * coefs / (n - 1))\n",
    "    return window\n",
    "\n",
    "def apply_preemphasis(y, preemCoef=0.97):\n",
    "    y[1:] = y[1:] - preemCoef*y[:-1]\n",
    "    y[0] *= (1 - preemCoef)\n",
    "    return y\n",
    "\n",
    "def freq_to_mel(freq):\n",
    "    return 2595.0 * np.log10(1.0 + freq / 700.0)\n",
    "\n",
    "def mel_to_freq(mels):\n",
    "    return 700.0 * (np.power(10.0, mels / 2595.0) - 1.0)\n",
    "\n",
    "@lru_cache(maxsize=10)\n",
    "def get_filterbank(numfilters, filterLen, lowFreq, highFreq, samplingFreq):\n",
    "    minwarpfreq = freq_to_mel(lowFreq)\n",
    "    maxwarpfreq = freq_to_mel(highFreq)\n",
    "    dwarp = (maxwarpfreq - minwarpfreq) / (numfilters + 1)\n",
    "    f = mel_to_freq(np.arange(numfilters + 2) * dwarp + minwarpfreq) * (filterLen - 1) * 2.0 / samplingFreq\n",
    "    i = np.arange(filterLen)[None, :]\n",
    "    f = f[:, None]\n",
    "    hislope = (i - f[:numfilters]) / (f[1:numfilters+1] - f[:numfilters])\n",
    "    loslope = (f[2:numfilters+2] - i) / (f[2:numfilters+2] - f[1:numfilters+1])\n",
    "    H = np.maximum(0, np.minimum(hislope, loslope))\n",
    "    return H\n",
    "\n",
    "def normalized(y, threshold=0):\n",
    "    y -= y.mean()\n",
    "    stddev = y.std()\n",
    "    if stddev > threshold:\n",
    "        y /= stddev\n",
    "    return y\n",
    "\n",
    "def mfsc(y, sfr, window_size=0.025, window_stride=0.010, window='hamming', normalize=True, log=True, n_mels=80, preemCoef=0.97, melfloor=1.0):\n",
    "    win_length = int(sfr * window_size)\n",
    "    hop_length = int(sfr * window_stride)\n",
    "    n_fft = 2048\n",
    "    lowfreq = 0\n",
    "    highfreq = sfr/2\n",
    "    \n",
    "    # get window\n",
    "    window = get_window(win_length)\n",
    "    padded_window = np.pad(window, (0, n_fft - win_length), mode='constant')[:, None]\n",
    "    \n",
    "    # preemphasis\n",
    "    y = apply_preemphasis(y, preemCoef)\n",
    "\n",
    "    # scale wave signal\n",
    "    y *= 32768\n",
    "    \n",
    "    # get frames and scale input\n",
    "    num_frames = 1 + (len(y) - win_length) // hop_length\n",
    "    pad_after = num_frames*hop_length + (n_fft - hop_length) - len(y)\n",
    "    if pad_after > 0:\n",
    "        y = np.pad(y, (0, pad_after), mode='constant')\n",
    "    frames = np.lib.stride_tricks.as_strided(y, shape=(n_fft, num_frames), strides=(y.itemsize, hop_length * y.itemsize), writeable=False)\n",
    "    windowed_frames = padded_window * frames\n",
    "    D = np.abs(np.fft.rfft(windowed_frames, axis=0))\n",
    "\n",
    "    # mel filterbank\n",
    "    filterbank = get_filterbank(n_mels, n_fft/2 + 1, lowfreq, highfreq, sfr)\n",
    "    mf = np.dot(filterbank, D)\n",
    "    mf = np.maximum(melfloor, mf)\n",
    "    if log:\n",
    "        mf = np.log(mf)\n",
    "    if normalize:\n",
    "        mf = normalized(mf)\n",
    "\n",
    "    return mf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5102cf2a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-13T22:00:47.465292Z",
     "iopub.status.busy": "2023-06-13T22:00:47.465000Z",
     "iopub.status.idle": "2023-06-13T22:00:47.472893Z",
     "shell.execute_reply": "2023-06-13T22:00:47.471967Z"
    },
    "papermill": {
     "duration": 0.016074,
     "end_time": "2023-06-13T22:00:47.474990",
     "exception": false,
     "start_time": "2023-06-13T22:00:47.458916",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_dataset(kaldi_path, class_to_id):\n",
    "    text_path = os.path.join(kaldi_path, 'text')\n",
    "    wav_path = os.path.join(kaldi_path, 'wav.scp')\n",
    "\n",
    "    key_to_word = dict()\n",
    "    key_to_wav = dict()\n",
    "    \n",
    "    with open(wav_path, 'rt') as wav_scp:\n",
    "        for line in wav_scp:\n",
    "            key, wav = line.strip().split(' ', 1)\n",
    "            key_to_wav[key] = wav\n",
    "            key_to_word[key] = None # default\n",
    "\n",
    "    if os.path.isfile(text_path):\n",
    "        with open(text_path, 'rt') as text:\n",
    "            for line in text:\n",
    "                key, word = line.strip().split(' ', 1)\n",
    "                key_to_word[key] = word\n",
    "\n",
    "    wavs = []\n",
    "    for key, wav_command in key_to_wav.items():\n",
    "        word = key_to_word[key]\n",
    "        word_id = class_to_id[word] if word is not None else -1 # default for test\n",
    "        wav_item = [key, wav_command, word_id]\n",
    "        wavs.append(wav_item)\n",
    "\n",
    "    return wavs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bfa4423",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-13T22:00:47.486278Z",
     "iopub.status.busy": "2023-06-13T22:00:47.485789Z",
     "iopub.status.idle": "2023-06-13T22:00:47.491499Z",
     "shell.execute_reply": "2023-06-13T22:00:47.490491Z"
    },
    "papermill": {
     "duration": 0.013537,
     "end_time": "2023-06-13T22:00:47.493458",
     "exception": false,
     "start_time": "2023-06-13T22:00:47.479921",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def wav_read(path):\n",
    "    sr, y = scipy.io.wavfile.read(path)\n",
    "    y = y/32768 # Normalize to -1..1\n",
    "    y -= y.mean()\n",
    "    return y, sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6a43888",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-13T22:00:47.506192Z",
     "iopub.status.busy": "2023-06-13T22:00:47.504773Z",
     "iopub.status.idle": "2023-06-13T22:00:47.511899Z",
     "shell.execute_reply": "2023-06-13T22:00:47.511044Z"
    },
    "papermill": {
     "duration": 0.015381,
     "end_time": "2023-06-13T22:00:47.514021",
     "exception": false,
     "start_time": "2023-06-13T22:00:47.498640",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def param_loader(path, window_size, window_stride, window, normalize, max_len):\n",
    "    y, sfr = wav_read(path)\n",
    "\n",
    "    param = mfsc(y, sfr, window_size=window_size, window_stride=window_stride, window=window, normalize=normalize, log=False, n_mels=60, preemCoef=0, melfloor=1.0)\n",
    "\n",
    "    # Add zero padding to make all param with the same dims\n",
    "    if param.shape[1] < max_len:\n",
    "        pad = np.zeros((param.shape[0], max_len - param.shape[1]))\n",
    "        param = np.hstack((pad, param))\n",
    "\n",
    "    # If exceeds max_len keep last samples\n",
    "    elif param.shape[1] > max_len:\n",
    "        param = param[:, -max_len:]\n",
    "\n",
    "    param = torch.FloatTensor(param)\n",
    "\n",
    "    return param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d758334",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-13T22:00:47.525758Z",
     "iopub.status.busy": "2023-06-13T22:00:47.524914Z",
     "iopub.status.idle": "2023-06-13T22:00:47.529977Z",
     "shell.execute_reply": "2023-06-13T22:00:47.529160Z"
    },
    "papermill": {
     "duration": 0.012739,
     "end_time": "2023-06-13T22:00:47.531852",
     "exception": false,
     "start_time": "2023-06-13T22:00:47.519113",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_classes():\n",
    "    classes = ['neg', 'pos']\n",
    "    weight = None\n",
    "    class_to_id = {label: i for i, label in enumerate(classes)}\n",
    "    return classes, weight, class_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6373d106",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-13T22:00:47.543024Z",
     "iopub.status.busy": "2023-06-13T22:00:47.542771Z",
     "iopub.status.idle": "2023-06-13T22:00:47.553025Z",
     "shell.execute_reply": "2023-06-13T22:00:47.552205Z"
    },
    "papermill": {
     "duration": 0.018262,
     "end_time": "2023-06-13T22:00:47.555093",
     "exception": false,
     "start_time": "2023-06-13T22:00:47.536831",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Loader(data.Dataset):\n",
    "    \"\"\"Data set loader::\n",
    "    Args:\n",
    "        root (string): Kaldi directory path.\n",
    "        transform (callable, optional): A function/transform that takes in a spectrogram\n",
    "            and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
    "        target_transform (callable, optional): A function/transform that takes in the\n",
    "            target and transforms it.\n",
    "        window_size: window size for the stft, default value is .02\n",
    "        window_stride: window stride for the stft, default value is .01\n",
    "        window_type: typye of window to extract the stft, default value is 'hamming'\n",
    "        normalize: boolean, whether or not to normalize the param to have zero mean and one std\n",
    "        max_len: the maximum length of frames to use\n",
    "     Attributes:\n",
    "        classes (list): List of the class names.\n",
    "        class_to_id (dict): Dict with items (class_name, class_index).\n",
    "        wavs (list): List of (wavs path, class_index) tuples\n",
    "        STFT parameters: window_size, window_stride, window_type, normalize\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root, transform=None, target_transform=None, window_size=.02,\n",
    "                 window_stride=.01, window_type='hamming', normalize=True, max_len=1000):\n",
    "\n",
    "        classes, weight, class_to_id = get_classes()\n",
    "        self.root = root\n",
    "        self.wavs = make_dataset(root, class_to_id)\n",
    "        self.classes = classes\n",
    "        self.weight = weight\n",
    "        self.class_to_id = class_to_id\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.loader = param_loader\n",
    "        self.window_size = window_size\n",
    "        self.window_stride = window_stride\n",
    "        self.window_type = window_type\n",
    "        self.normalize = normalize\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "        Returns:\n",
    "            tuple: (key, params, target) where target is class_index of the target class.\n",
    "        \"\"\"\n",
    "        key, path, target = self.wavs[index]\n",
    "        path = '../input/covid3/wavs16k/' + path\n",
    "        params = self.loader(path, self.window_size, self.window_stride, self.window_type, self.normalize, self.max_len)  # pylint: disable=line-too-long\n",
    "        if self.transform is not None:\n",
    "            params = self.transform(params)\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return key, params, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.wavs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "680f773e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-13T22:00:47.566495Z",
     "iopub.status.busy": "2023-06-13T22:00:47.565745Z",
     "iopub.status.idle": "2023-06-13T22:00:47.581862Z",
     "shell.execute_reply": "2023-06-13T22:00:47.581038Z"
    },
    "id": "79opq8kbeIQ9",
    "papermill": {
     "duration": 0.0243,
     "end_time": "2023-06-13T22:00:47.584277",
     "exception": false,
     "start_time": "2023-06-13T22:00:47.559977",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class VGG(nn.Module):\n",
    "\n",
    "    def __init__(self, vgg_name, hidden=64, dropout=0.4):\n",
    "        super(VGG, self).__init__()\n",
    "        self.features = make_layers(cfg[vgg_name])\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(2*512, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden, 1),\n",
    "        )\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x.unsqueeze_(1)\n",
    "        x = self.features(x)\n",
    "        x1, _ = x.max(dim=-1)\n",
    "        x2 = x.mean(dim=-1)\n",
    "        x = torch.cat((x1, x2), dim=-1)\n",
    "        # print(x.shape)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x.squeeze(-1)\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                m.weight.data.normal_(0, 0.01)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "\n",
    "def make_layers(cfg, batch_norm=True):\n",
    "    layers = []\n",
    "    in_channels = 1\n",
    "    for v in cfg:\n",
    "        if v == 'M':\n",
    "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "        else:\n",
    "            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
    "            if batch_norm:\n",
    "                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
    "            else:\n",
    "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "            in_channels = v\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "cfg = {\n",
    "    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "116ee930",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-13T22:00:47.595671Z",
     "iopub.status.busy": "2023-06-13T22:00:47.594938Z",
     "iopub.status.idle": "2023-06-13T22:00:47.602446Z",
     "shell.execute_reply": "2023-06-13T22:00:47.601568Z"
    },
    "papermill": {
     "duration": 0.01514,
     "end_time": "2023-06-13T22:00:47.604360",
     "exception": false,
     "start_time": "2023-06-13T22:00:47.589220",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(loader, model, criterion, optimizer, epoch, cuda, log_interval, weight=None, verbose=True):\n",
    "    model.train()\n",
    "    global_epoch_loss = 0\n",
    "    samples = 0\n",
    "    for batch_idx, (_, data, target) in enumerate(loader):\n",
    "        if cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        global_epoch_loss += loss.data.item() * len(target)\n",
    "        samples += len(target)\n",
    "        if verbose:\n",
    "            if batch_idx % log_interval == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, samples, len(loader.dataset), 100 * samples / len(loader.dataset), global_epoch_loss / samples))\n",
    "    return global_epoch_loss / samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a55110d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-13T22:00:47.615688Z",
     "iopub.status.busy": "2023-06-13T22:00:47.615015Z",
     "iopub.status.idle": "2023-06-13T22:00:47.624139Z",
     "shell.execute_reply": "2023-06-13T22:00:47.623303Z"
    },
    "papermill": {
     "duration": 0.017057,
     "end_time": "2023-06-13T22:00:47.626376",
     "exception": false,
     "start_time": "2023-06-13T22:00:47.609319",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test(loader, model, criterion, cuda, verbose=True, data_set='Test', save=None):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    tpred = []\n",
    "    ttarget = []\n",
    "\n",
    "    if save is not None:\n",
    "        csv = open(save, 'wt')\n",
    "        print('index,prob', file=csv)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for keys, data, target in loader:\n",
    "            if cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            output = model(data)\n",
    "            pred = output.sigmoid()\n",
    "            tpred.append(pred.cpu().numpy())\n",
    "\n",
    "            if target[0] != -1:\n",
    "                loss = criterion(output, target.float()).data.item()\n",
    "                test_loss += loss * len(target) # sum up batch loss \n",
    "                ttarget.append(target.cpu().numpy())\n",
    "\n",
    "            if save is not None:\n",
    "                for i, key in enumerate(keys):\n",
    "                    print(f'{key},{pred[i]}', file=csv)\n",
    "    \n",
    "    if len(ttarget) > 0:\n",
    "        test_loss /= len(loader.dataset)\n",
    "        auc = roc_auc_score(np.concatenate(ttarget), np.concatenate(tpred))\n",
    "        if verbose:\n",
    "            print('\\n{} set: Average loss: {:.4f}, AUC: ({:.1f}%)\\n'.format(data_set, test_loss, 100 * auc))\n",
    "\n",
    "        return test_loss, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4175d837",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-13T22:00:47.637813Z",
     "iopub.status.busy": "2023-06-13T22:00:47.637024Z",
     "iopub.status.idle": "2023-06-13T22:00:47.643631Z",
     "shell.execute_reply": "2023-06-13T22:00:47.642870Z"
    },
    "papermill": {
     "duration": 0.014259,
     "end_time": "2023-06-13T22:00:47.645633",
     "exception": false,
     "start_time": "2023-06-13T22:00:47.631374",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "args = SimpleNamespace(\n",
    "    # general options\n",
    "    train_path = '../input/covid3/train',        # train data folder\n",
    "    valid_path = '../input/covid3/valid',        # valid data folder\n",
    "    test_path = '../input/covid3/test',          # test data folder\n",
    "    batch_size = 32,                             # training and valid batch size\n",
    "    test_batch_size = 32,                        # batch size for testing\n",
    "    arc = 'VGG13',                               # VGG11, VGG13, VGG16, VGG19\n",
    "    epochs = 100,                                # maximum number of epochs to train\n",
    "    lr = 0.0005,                                 # learning rate\n",
    "    momentum = 0.9,                              # SGD momentum, for SGD only\n",
    "    optimizer = 'adam',                          # optimization method: sgd | adam\n",
    "    seed = 1234,                                 # random seed\n",
    "    log_interval = 5,                            # how many batches to wait before logging training status\n",
    "    patience = 5,                                # how many epochs of no loss improvement should we wait before stop training\n",
    "    checkpoint = '.',                            # checkpoints directory\n",
    "    train = True,                                # train before testing\n",
    "    cuda = True,                                 # use gpu\n",
    "\n",
    "    # feature extraction options\n",
    "    window_size = .04,                           # window size for the stft\n",
    "    window_stride = .02,                         # window stride for the stft\n",
    "    window_type = 'hamming',                     # window type for the stft\n",
    "    normalize = True,                            # use spect normalization\n",
    "    num_workers = 2,                             # how many subprocesses to use for data loading\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f15911e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-13T22:00:47.656979Z",
     "iopub.status.busy": "2023-06-13T22:00:47.656228Z",
     "iopub.status.idle": "2023-06-13T22:00:50.622728Z",
     "shell.execute_reply": "2023-06-13T22:00:50.621760Z"
    },
    "papermill": {
     "duration": 2.974785,
     "end_time": "2023-06-13T22:00:50.625343",
     "exception": false,
     "start_time": "2023-06-13T22:00:47.650558",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA with 1 GPUs\n"
     ]
    }
   ],
   "source": [
    "args.cuda = args.cuda and torch.cuda.is_available()\n",
    "torch.manual_seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "    print('Using CUDA with {0} GPUs'.format(torch.cuda.device_count()))\n",
    "\n",
    "\n",
    "# build model\n",
    "model = VGG(args.arc)\n",
    "if args.cuda:\n",
    "    model.cuda()\n",
    "\n",
    "# Define criterion\n",
    "criterion = nn.BCEWithLogitsLoss(reduction='mean') # This loss combines a Sigmoid layer and the BCELoss in one single class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ed668d",
   "metadata": {
    "papermill": {
     "duration": 0.004987,
     "end_time": "2023-06-13T22:00:50.635787",
     "exception": false,
     "start_time": "2023-06-13T22:00:50.630800",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f5c8756",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-13T22:00:50.647270Z",
     "iopub.status.busy": "2023-06-13T22:00:50.646975Z",
     "iopub.status.idle": "2023-06-13T22:21:36.817150Z",
     "shell.execute_reply": "2023-06-13T22:21:36.815981Z"
    },
    "papermill": {
     "duration": 1246.203461,
     "end_time": "2023-06-13T22:21:36.844326",
     "exception": false,
     "start_time": "2023-06-13T22:00:50.640865",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [32/2160 (1%)]\tLoss: 0.693231\n",
      "Train Epoch: 1 [192/2160 (9%)]\tLoss: 0.693242\n",
      "Train Epoch: 1 [352/2160 (16%)]\tLoss: 0.691858\n",
      "Train Epoch: 1 [512/2160 (24%)]\tLoss: 0.693454\n",
      "Train Epoch: 1 [672/2160 (31%)]\tLoss: 0.693392\n",
      "Train Epoch: 1 [832/2160 (39%)]\tLoss: 0.692011\n",
      "Train Epoch: 1 [992/2160 (46%)]\tLoss: 0.691441\n",
      "Train Epoch: 1 [1152/2160 (53%)]\tLoss: 0.691586\n",
      "Train Epoch: 1 [1312/2160 (61%)]\tLoss: 0.690815\n",
      "Train Epoch: 1 [1472/2160 (68%)]\tLoss: 0.690531\n",
      "Train Epoch: 1 [1632/2160 (76%)]\tLoss: 0.689095\n",
      "Train Epoch: 1 [1792/2160 (83%)]\tLoss: 0.690117\n",
      "Train Epoch: 1 [1952/2160 (90%)]\tLoss: 0.689751\n",
      "Train Epoch: 1 [2112/2160 (98%)]\tLoss: 0.689800\n",
      "\n",
      "Validation set: Average loss: 0.6840, AUC: (61.6%)\n",
      "\n",
      "Saving state\n",
      "Elapsed seconds: (80s)\n",
      "Train Epoch: 2 [32/2160 (1%)]\tLoss: 0.679039\n",
      "Train Epoch: 2 [192/2160 (9%)]\tLoss: 0.697365\n",
      "Train Epoch: 2 [352/2160 (16%)]\tLoss: 0.692832\n",
      "Train Epoch: 2 [512/2160 (24%)]\tLoss: 0.692115\n",
      "Train Epoch: 2 [672/2160 (31%)]\tLoss: 0.691173\n",
      "Train Epoch: 2 [832/2160 (39%)]\tLoss: 0.691536\n",
      "Train Epoch: 2 [992/2160 (46%)]\tLoss: 0.689424\n",
      "Train Epoch: 2 [1152/2160 (53%)]\tLoss: 0.688375\n",
      "Train Epoch: 2 [1312/2160 (61%)]\tLoss: 0.684877\n",
      "Train Epoch: 2 [1472/2160 (68%)]\tLoss: 0.685878\n",
      "Train Epoch: 2 [1632/2160 (76%)]\tLoss: 0.681793\n",
      "Train Epoch: 2 [1792/2160 (83%)]\tLoss: 0.682418\n",
      "Train Epoch: 2 [1952/2160 (90%)]\tLoss: 0.684414\n",
      "Train Epoch: 2 [2112/2160 (98%)]\tLoss: 0.682641\n",
      "\n",
      "Validation set: Average loss: 0.7351, AUC: (64.2%)\n",
      "\n",
      "Saving state\n",
      "Elapsed seconds: (150s)\n",
      "Train Epoch: 3 [32/2160 (1%)]\tLoss: 0.670506\n",
      "Train Epoch: 3 [192/2160 (9%)]\tLoss: 0.683884\n",
      "Train Epoch: 3 [352/2160 (16%)]\tLoss: 0.681538\n",
      "Train Epoch: 3 [512/2160 (24%)]\tLoss: 0.681159\n",
      "Train Epoch: 3 [672/2160 (31%)]\tLoss: 0.682318\n",
      "Train Epoch: 3 [832/2160 (39%)]\tLoss: 0.679573\n",
      "Train Epoch: 3 [992/2160 (46%)]\tLoss: 0.675405\n",
      "Train Epoch: 3 [1152/2160 (53%)]\tLoss: 0.675775\n",
      "Train Epoch: 3 [1312/2160 (61%)]\tLoss: 0.672852\n",
      "Train Epoch: 3 [1472/2160 (68%)]\tLoss: 0.671781\n",
      "Train Epoch: 3 [1632/2160 (76%)]\tLoss: 0.672753\n",
      "Train Epoch: 3 [1792/2160 (83%)]\tLoss: 0.674236\n",
      "Train Epoch: 3 [1952/2160 (90%)]\tLoss: 0.674661\n",
      "Train Epoch: 3 [2112/2160 (98%)]\tLoss: 0.676066\n",
      "\n",
      "Validation set: Average loss: 0.6741, AUC: (63.5%)\n",
      "\n",
      "AUC was not improved, iteration 1\n",
      "Elapsed seconds: (220s)\n",
      "Train Epoch: 4 [32/2160 (1%)]\tLoss: 0.713355\n",
      "Train Epoch: 4 [192/2160 (9%)]\tLoss: 0.677971\n",
      "Train Epoch: 4 [352/2160 (16%)]\tLoss: 0.679084\n",
      "Train Epoch: 4 [512/2160 (24%)]\tLoss: 0.676057\n",
      "Train Epoch: 4 [672/2160 (31%)]\tLoss: 0.675942\n",
      "Train Epoch: 4 [832/2160 (39%)]\tLoss: 0.673318\n",
      "Train Epoch: 4 [992/2160 (46%)]\tLoss: 0.672418\n",
      "Train Epoch: 4 [1152/2160 (53%)]\tLoss: 0.673009\n",
      "Train Epoch: 4 [1312/2160 (61%)]\tLoss: 0.674450\n",
      "Train Epoch: 4 [1472/2160 (68%)]\tLoss: 0.675068\n",
      "Train Epoch: 4 [1632/2160 (76%)]\tLoss: 0.673109\n",
      "Train Epoch: 4 [1792/2160 (83%)]\tLoss: 0.672849\n",
      "Train Epoch: 4 [1952/2160 (90%)]\tLoss: 0.674359\n",
      "Train Epoch: 4 [2112/2160 (98%)]\tLoss: 0.674440\n",
      "\n",
      "Validation set: Average loss: 0.7262, AUC: (63.7%)\n",
      "\n",
      "AUC was not improved, iteration 2\n",
      "Elapsed seconds: (289s)\n",
      "Train Epoch: 5 [32/2160 (1%)]\tLoss: 0.702960\n",
      "Train Epoch: 5 [192/2160 (9%)]\tLoss: 0.666028\n",
      "Train Epoch: 5 [352/2160 (16%)]\tLoss: 0.662391\n",
      "Train Epoch: 5 [512/2160 (24%)]\tLoss: 0.665629\n",
      "Train Epoch: 5 [672/2160 (31%)]\tLoss: 0.669002\n",
      "Train Epoch: 5 [832/2160 (39%)]\tLoss: 0.672418\n",
      "Train Epoch: 5 [992/2160 (46%)]\tLoss: 0.672447\n",
      "Train Epoch: 5 [1152/2160 (53%)]\tLoss: 0.670817\n",
      "Train Epoch: 5 [1312/2160 (61%)]\tLoss: 0.668803\n",
      "Train Epoch: 5 [1472/2160 (68%)]\tLoss: 0.665796\n",
      "Train Epoch: 5 [1632/2160 (76%)]\tLoss: 0.665005\n",
      "Train Epoch: 5 [1792/2160 (83%)]\tLoss: 0.668287\n",
      "Train Epoch: 5 [1952/2160 (90%)]\tLoss: 0.668369\n",
      "Train Epoch: 5 [2112/2160 (98%)]\tLoss: 0.668484\n",
      "\n",
      "Validation set: Average loss: 0.6616, AUC: (64.9%)\n",
      "\n",
      "Saving state\n",
      "Elapsed seconds: (360s)\n",
      "Train Epoch: 6 [32/2160 (1%)]\tLoss: 0.691253\n",
      "Train Epoch: 6 [192/2160 (9%)]\tLoss: 0.673482\n",
      "Train Epoch: 6 [352/2160 (16%)]\tLoss: 0.661519\n",
      "Train Epoch: 6 [512/2160 (24%)]\tLoss: 0.658670\n",
      "Train Epoch: 6 [672/2160 (31%)]\tLoss: 0.657899\n",
      "Train Epoch: 6 [832/2160 (39%)]\tLoss: 0.655828\n",
      "Train Epoch: 6 [992/2160 (46%)]\tLoss: 0.658689\n",
      "Train Epoch: 6 [1152/2160 (53%)]\tLoss: 0.656172\n",
      "Train Epoch: 6 [1312/2160 (61%)]\tLoss: 0.657759\n",
      "Train Epoch: 6 [1472/2160 (68%)]\tLoss: 0.659619\n",
      "Train Epoch: 6 [1632/2160 (76%)]\tLoss: 0.658548\n",
      "Train Epoch: 6 [1792/2160 (83%)]\tLoss: 0.659627\n",
      "Train Epoch: 6 [1952/2160 (90%)]\tLoss: 0.661264\n",
      "Train Epoch: 6 [2112/2160 (98%)]\tLoss: 0.661465\n",
      "\n",
      "Validation set: Average loss: 0.6676, AUC: (64.6%)\n",
      "\n",
      "AUC was not improved, iteration 1\n",
      "Elapsed seconds: (428s)\n",
      "Train Epoch: 7 [32/2160 (1%)]\tLoss: 0.668232\n",
      "Train Epoch: 7 [192/2160 (9%)]\tLoss: 0.667819\n",
      "Train Epoch: 7 [352/2160 (16%)]\tLoss: 0.674880\n",
      "Train Epoch: 7 [512/2160 (24%)]\tLoss: 0.666170\n",
      "Train Epoch: 7 [672/2160 (31%)]\tLoss: 0.666408\n",
      "Train Epoch: 7 [832/2160 (39%)]\tLoss: 0.662649\n",
      "Train Epoch: 7 [992/2160 (46%)]\tLoss: 0.660993\n",
      "Train Epoch: 7 [1152/2160 (53%)]\tLoss: 0.661807\n",
      "Train Epoch: 7 [1312/2160 (61%)]\tLoss: 0.657438\n",
      "Train Epoch: 7 [1472/2160 (68%)]\tLoss: 0.658995\n",
      "Train Epoch: 7 [1632/2160 (76%)]\tLoss: 0.658393\n",
      "Train Epoch: 7 [1792/2160 (83%)]\tLoss: 0.657228\n",
      "Train Epoch: 7 [1952/2160 (90%)]\tLoss: 0.658116\n",
      "Train Epoch: 7 [2112/2160 (98%)]\tLoss: 0.658381\n",
      "\n",
      "Validation set: Average loss: 0.7064, AUC: (62.3%)\n",
      "\n",
      "AUC was not improved, iteration 2\n",
      "Elapsed seconds: (495s)\n",
      "Train Epoch: 8 [32/2160 (1%)]\tLoss: 0.663160\n",
      "Train Epoch: 8 [192/2160 (9%)]\tLoss: 0.646619\n",
      "Train Epoch: 8 [352/2160 (16%)]\tLoss: 0.655913\n",
      "Train Epoch: 8 [512/2160 (24%)]\tLoss: 0.656080\n",
      "Train Epoch: 8 [672/2160 (31%)]\tLoss: 0.655826\n",
      "Train Epoch: 8 [832/2160 (39%)]\tLoss: 0.656320\n",
      "Train Epoch: 8 [992/2160 (46%)]\tLoss: 0.662200\n",
      "Train Epoch: 8 [1152/2160 (53%)]\tLoss: 0.661387\n",
      "Train Epoch: 8 [1312/2160 (61%)]\tLoss: 0.659016\n",
      "Train Epoch: 8 [1472/2160 (68%)]\tLoss: 0.661479\n",
      "Train Epoch: 8 [1632/2160 (76%)]\tLoss: 0.660441\n",
      "Train Epoch: 8 [1792/2160 (83%)]\tLoss: 0.660581\n",
      "Train Epoch: 8 [1952/2160 (90%)]\tLoss: 0.659453\n",
      "Train Epoch: 8 [2112/2160 (98%)]\tLoss: 0.659867\n",
      "\n",
      "Validation set: Average loss: 0.6898, AUC: (65.0%)\n",
      "\n",
      "Saving state\n",
      "Elapsed seconds: (562s)\n",
      "Train Epoch: 9 [32/2160 (1%)]\tLoss: 0.671995\n",
      "Train Epoch: 9 [192/2160 (9%)]\tLoss: 0.657287\n",
      "Train Epoch: 9 [352/2160 (16%)]\tLoss: 0.663271\n",
      "Train Epoch: 9 [512/2160 (24%)]\tLoss: 0.650483\n",
      "Train Epoch: 9 [672/2160 (31%)]\tLoss: 0.655792\n",
      "Train Epoch: 9 [832/2160 (39%)]\tLoss: 0.650959\n",
      "Train Epoch: 9 [992/2160 (46%)]\tLoss: 0.655423\n",
      "Train Epoch: 9 [1152/2160 (53%)]\tLoss: 0.655401\n",
      "Train Epoch: 9 [1312/2160 (61%)]\tLoss: 0.653632\n",
      "Train Epoch: 9 [1472/2160 (68%)]\tLoss: 0.651807\n",
      "Train Epoch: 9 [1632/2160 (76%)]\tLoss: 0.650830\n",
      "Train Epoch: 9 [1792/2160 (83%)]\tLoss: 0.649936\n",
      "Train Epoch: 9 [1952/2160 (90%)]\tLoss: 0.650572\n",
      "Train Epoch: 9 [2112/2160 (98%)]\tLoss: 0.653687\n",
      "\n",
      "Validation set: Average loss: 0.7164, AUC: (64.9%)\n",
      "\n",
      "AUC was not improved, iteration 1\n",
      "Elapsed seconds: (629s)\n",
      "Train Epoch: 10 [32/2160 (1%)]\tLoss: 0.553859\n",
      "Train Epoch: 10 [192/2160 (9%)]\tLoss: 0.658768\n",
      "Train Epoch: 10 [352/2160 (16%)]\tLoss: 0.636235\n",
      "Train Epoch: 10 [512/2160 (24%)]\tLoss: 0.649242\n",
      "Train Epoch: 10 [672/2160 (31%)]\tLoss: 0.651857\n",
      "Train Epoch: 10 [832/2160 (39%)]\tLoss: 0.646899\n",
      "Train Epoch: 10 [992/2160 (46%)]\tLoss: 0.643502\n",
      "Train Epoch: 10 [1152/2160 (53%)]\tLoss: 0.649012\n",
      "Train Epoch: 10 [1312/2160 (61%)]\tLoss: 0.643960\n",
      "Train Epoch: 10 [1472/2160 (68%)]\tLoss: 0.647332\n",
      "Train Epoch: 10 [1632/2160 (76%)]\tLoss: 0.652997\n",
      "Train Epoch: 10 [1792/2160 (83%)]\tLoss: 0.652663\n",
      "Train Epoch: 10 [1952/2160 (90%)]\tLoss: 0.651617\n",
      "Train Epoch: 10 [2112/2160 (98%)]\tLoss: 0.652061\n",
      "\n",
      "Validation set: Average loss: 0.6625, AUC: (65.5%)\n",
      "\n",
      "Saving state\n",
      "Elapsed seconds: (697s)\n",
      "Train Epoch: 11 [32/2160 (1%)]\tLoss: 0.663791\n",
      "Train Epoch: 11 [192/2160 (9%)]\tLoss: 0.640248\n",
      "Train Epoch: 11 [352/2160 (16%)]\tLoss: 0.655478\n",
      "Train Epoch: 11 [512/2160 (24%)]\tLoss: 0.649764\n",
      "Train Epoch: 11 [672/2160 (31%)]\tLoss: 0.646681\n",
      "Train Epoch: 11 [832/2160 (39%)]\tLoss: 0.648435\n",
      "Train Epoch: 11 [992/2160 (46%)]\tLoss: 0.643422\n",
      "Train Epoch: 11 [1152/2160 (53%)]\tLoss: 0.643707\n",
      "Train Epoch: 11 [1312/2160 (61%)]\tLoss: 0.642962\n",
      "Train Epoch: 11 [1472/2160 (68%)]\tLoss: 0.645489\n",
      "Train Epoch: 11 [1632/2160 (76%)]\tLoss: 0.647381\n",
      "Train Epoch: 11 [1792/2160 (83%)]\tLoss: 0.648971\n",
      "Train Epoch: 11 [1952/2160 (90%)]\tLoss: 0.648919\n",
      "Train Epoch: 11 [2112/2160 (98%)]\tLoss: 0.646980\n",
      "\n",
      "Validation set: Average loss: 0.6754, AUC: (65.2%)\n",
      "\n",
      "AUC was not improved, iteration 1\n",
      "Elapsed seconds: (766s)\n",
      "Train Epoch: 12 [32/2160 (1%)]\tLoss: 0.701533\n",
      "Train Epoch: 12 [192/2160 (9%)]\tLoss: 0.650808\n",
      "Train Epoch: 12 [352/2160 (16%)]\tLoss: 0.642176\n",
      "Train Epoch: 12 [512/2160 (24%)]\tLoss: 0.651495\n",
      "Train Epoch: 12 [672/2160 (31%)]\tLoss: 0.662573\n",
      "Train Epoch: 12 [832/2160 (39%)]\tLoss: 0.649867\n",
      "Train Epoch: 12 [992/2160 (46%)]\tLoss: 0.645699\n",
      "Train Epoch: 12 [1152/2160 (53%)]\tLoss: 0.646354\n",
      "Train Epoch: 12 [1312/2160 (61%)]\tLoss: 0.645190\n",
      "Train Epoch: 12 [1472/2160 (68%)]\tLoss: 0.645667\n",
      "Train Epoch: 12 [1632/2160 (76%)]\tLoss: 0.644109\n",
      "Train Epoch: 12 [1792/2160 (83%)]\tLoss: 0.643576\n",
      "Train Epoch: 12 [1952/2160 (90%)]\tLoss: 0.641655\n",
      "Train Epoch: 12 [2112/2160 (98%)]\tLoss: 0.644518\n",
      "\n",
      "Validation set: Average loss: 0.6637, AUC: (65.2%)\n",
      "\n",
      "AUC was not improved, iteration 2\n",
      "Elapsed seconds: (832s)\n",
      "Train Epoch: 13 [32/2160 (1%)]\tLoss: 0.624704\n",
      "Train Epoch: 13 [192/2160 (9%)]\tLoss: 0.603805\n",
      "Train Epoch: 13 [352/2160 (16%)]\tLoss: 0.618679\n",
      "Train Epoch: 13 [512/2160 (24%)]\tLoss: 0.624393\n",
      "Train Epoch: 13 [672/2160 (31%)]\tLoss: 0.626873\n",
      "Train Epoch: 13 [832/2160 (39%)]\tLoss: 0.627556\n",
      "Train Epoch: 13 [992/2160 (46%)]\tLoss: 0.629229\n",
      "Train Epoch: 13 [1152/2160 (53%)]\tLoss: 0.627430\n",
      "Train Epoch: 13 [1312/2160 (61%)]\tLoss: 0.631804\n",
      "Train Epoch: 13 [1472/2160 (68%)]\tLoss: 0.634749\n",
      "Train Epoch: 13 [1632/2160 (76%)]\tLoss: 0.636272\n",
      "Train Epoch: 13 [1792/2160 (83%)]\tLoss: 0.637349\n",
      "Train Epoch: 13 [1952/2160 (90%)]\tLoss: 0.637848\n",
      "Train Epoch: 13 [2112/2160 (98%)]\tLoss: 0.637904\n",
      "\n",
      "Validation set: Average loss: 0.7018, AUC: (65.9%)\n",
      "\n",
      "Saving state\n",
      "Elapsed seconds: (901s)\n",
      "Train Epoch: 14 [32/2160 (1%)]\tLoss: 0.671615\n",
      "Train Epoch: 14 [192/2160 (9%)]\tLoss: 0.651600\n",
      "Train Epoch: 14 [352/2160 (16%)]\tLoss: 0.639220\n",
      "Train Epoch: 14 [512/2160 (24%)]\tLoss: 0.642060\n",
      "Train Epoch: 14 [672/2160 (31%)]\tLoss: 0.640988\n",
      "Train Epoch: 14 [832/2160 (39%)]\tLoss: 0.633553\n",
      "Train Epoch: 14 [992/2160 (46%)]\tLoss: 0.629311\n",
      "Train Epoch: 14 [1152/2160 (53%)]\tLoss: 0.627672\n",
      "Train Epoch: 14 [1312/2160 (61%)]\tLoss: 0.626367\n",
      "Train Epoch: 14 [1472/2160 (68%)]\tLoss: 0.627827\n",
      "Train Epoch: 14 [1632/2160 (76%)]\tLoss: 0.631014\n",
      "Train Epoch: 14 [1792/2160 (83%)]\tLoss: 0.629111\n",
      "Train Epoch: 14 [1952/2160 (90%)]\tLoss: 0.628892\n",
      "Train Epoch: 14 [2112/2160 (98%)]\tLoss: 0.632058\n",
      "\n",
      "Validation set: Average loss: 0.6956, AUC: (65.2%)\n",
      "\n",
      "AUC was not improved, iteration 1\n",
      "Elapsed seconds: (972s)\n",
      "Train Epoch: 15 [32/2160 (1%)]\tLoss: 0.680978\n",
      "Train Epoch: 15 [192/2160 (9%)]\tLoss: 0.608239\n",
      "Train Epoch: 15 [352/2160 (16%)]\tLoss: 0.606369\n",
      "Train Epoch: 15 [512/2160 (24%)]\tLoss: 0.607748\n",
      "Train Epoch: 15 [672/2160 (31%)]\tLoss: 0.605304\n",
      "Train Epoch: 15 [832/2160 (39%)]\tLoss: 0.605175\n",
      "Train Epoch: 15 [992/2160 (46%)]\tLoss: 0.598445\n",
      "Train Epoch: 15 [1152/2160 (53%)]\tLoss: 0.606579\n",
      "Train Epoch: 15 [1312/2160 (61%)]\tLoss: 0.609823\n",
      "Train Epoch: 15 [1472/2160 (68%)]\tLoss: 0.613061\n",
      "Train Epoch: 15 [1632/2160 (76%)]\tLoss: 0.616259\n",
      "Train Epoch: 15 [1792/2160 (83%)]\tLoss: 0.616537\n",
      "Train Epoch: 15 [1952/2160 (90%)]\tLoss: 0.619523\n",
      "Train Epoch: 15 [2112/2160 (98%)]\tLoss: 0.619896\n",
      "\n",
      "Validation set: Average loss: 0.7503, AUC: (64.7%)\n",
      "\n",
      "AUC was not improved, iteration 2\n",
      "Elapsed seconds: (1040s)\n",
      "Train Epoch: 16 [32/2160 (1%)]\tLoss: 0.619448\n",
      "Train Epoch: 16 [192/2160 (9%)]\tLoss: 0.611044\n",
      "Train Epoch: 16 [352/2160 (16%)]\tLoss: 0.644761\n",
      "Train Epoch: 16 [512/2160 (24%)]\tLoss: 0.631632\n",
      "Train Epoch: 16 [672/2160 (31%)]\tLoss: 0.622116\n",
      "Train Epoch: 16 [832/2160 (39%)]\tLoss: 0.620869\n",
      "Train Epoch: 16 [992/2160 (46%)]\tLoss: 0.623126\n",
      "Train Epoch: 16 [1152/2160 (53%)]\tLoss: 0.631189\n",
      "Train Epoch: 16 [1312/2160 (61%)]\tLoss: 0.628719\n",
      "Train Epoch: 16 [1472/2160 (68%)]\tLoss: 0.629316\n",
      "Train Epoch: 16 [1632/2160 (76%)]\tLoss: 0.626721\n",
      "Train Epoch: 16 [1792/2160 (83%)]\tLoss: 0.627923\n",
      "Train Epoch: 16 [1952/2160 (90%)]\tLoss: 0.624810\n",
      "Train Epoch: 16 [2112/2160 (98%)]\tLoss: 0.621900\n",
      "\n",
      "Validation set: Average loss: 0.6807, AUC: (63.3%)\n",
      "\n",
      "AUC was not improved, iteration 3\n",
      "Elapsed seconds: (1107s)\n",
      "Train Epoch: 17 [32/2160 (1%)]\tLoss: 0.503940\n",
      "Train Epoch: 17 [192/2160 (9%)]\tLoss: 0.597562\n",
      "Train Epoch: 17 [352/2160 (16%)]\tLoss: 0.582905\n",
      "Train Epoch: 17 [512/2160 (24%)]\tLoss: 0.569139\n",
      "Train Epoch: 17 [672/2160 (31%)]\tLoss: 0.577279\n",
      "Train Epoch: 17 [832/2160 (39%)]\tLoss: 0.580534\n",
      "Train Epoch: 17 [992/2160 (46%)]\tLoss: 0.590812\n",
      "Train Epoch: 17 [1152/2160 (53%)]\tLoss: 0.597000\n",
      "Train Epoch: 17 [1312/2160 (61%)]\tLoss: 0.600083\n",
      "Train Epoch: 17 [1472/2160 (68%)]\tLoss: 0.605060\n",
      "Train Epoch: 17 [1632/2160 (76%)]\tLoss: 0.610463\n",
      "Train Epoch: 17 [1792/2160 (83%)]\tLoss: 0.612885\n",
      "Train Epoch: 17 [1952/2160 (90%)]\tLoss: 0.614588\n",
      "Train Epoch: 17 [2112/2160 (98%)]\tLoss: 0.613682\n",
      "\n",
      "Validation set: Average loss: 0.6734, AUC: (64.9%)\n",
      "\n",
      "AUC was not improved, iteration 4\n",
      "Elapsed seconds: (1177s)\n",
      "Train Epoch: 18 [32/2160 (1%)]\tLoss: 0.670318\n",
      "Train Epoch: 18 [192/2160 (9%)]\tLoss: 0.601324\n",
      "Train Epoch: 18 [352/2160 (16%)]\tLoss: 0.580093\n",
      "Train Epoch: 18 [512/2160 (24%)]\tLoss: 0.591143\n",
      "Train Epoch: 18 [672/2160 (31%)]\tLoss: 0.587508\n",
      "Train Epoch: 18 [832/2160 (39%)]\tLoss: 0.585391\n",
      "Train Epoch: 18 [992/2160 (46%)]\tLoss: 0.588450\n",
      "Train Epoch: 18 [1152/2160 (53%)]\tLoss: 0.593334\n",
      "Train Epoch: 18 [1312/2160 (61%)]\tLoss: 0.598689\n",
      "Train Epoch: 18 [1472/2160 (68%)]\tLoss: 0.594670\n",
      "Train Epoch: 18 [1632/2160 (76%)]\tLoss: 0.594590\n",
      "Train Epoch: 18 [1792/2160 (83%)]\tLoss: 0.596052\n",
      "Train Epoch: 18 [1952/2160 (90%)]\tLoss: 0.591773\n",
      "Train Epoch: 18 [2112/2160 (98%)]\tLoss: 0.594550\n",
      "\n",
      "Validation set: Average loss: 0.7140, AUC: (65.0%)\n",
      "\n",
      "AUC was not improved, iteration 5\n",
      "Elapsed seconds: (1246s)\n"
     ]
    }
   ],
   "source": [
    "# loading data\n",
    "if args.train:\n",
    "    train_dataset = Loader(args.train_path, window_size=args.window_size, window_stride=args.window_stride,\n",
    "        window_type=args.window_type, normalize=args.normalize)\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=args.batch_size, shuffle=True,\n",
    "        num_workers=args.num_workers, pin_memory=args.cuda, sampler=None)\n",
    "\n",
    "    valid_dataset = Loader(args.valid_path, window_size=args.window_size, window_stride=args.window_stride,\n",
    "        window_type=args.window_type, normalize=args.normalize)\n",
    "    valid_loader = torch.utils.data.DataLoader(\n",
    "        valid_dataset, batch_size=args.batch_size, shuffle=None,\n",
    "        num_workers=args.num_workers, pin_memory=args.cuda, sampler=None)\n",
    "\n",
    "    # define optimizer\n",
    "    if args.optimizer.lower() == 'adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "    else:\n",
    "        optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)\n",
    "\n",
    "    best_valid_auc = 0\n",
    "    iteration = 0\n",
    "    epoch = 1\n",
    "\n",
    "    # trainint with early stopping\n",
    "    t0 = time.time()\n",
    "    while (epoch < args.epochs + 1) and (iteration < args.patience):\n",
    "        train(train_loader, model, criterion, optimizer, epoch, args.cuda, args.log_interval,\n",
    "            weight=train_dataset.weight)\n",
    "        valid_loss, valid_auc = test(valid_loader, model, criterion, args.cuda, data_set='Validation')\n",
    "        if not os.path.isdir(args.checkpoint):\n",
    "            os.mkdir(args.checkpoint)\n",
    "        torch.save(model.state_dict(), './{}/model{:03d}.pt'.format(args.checkpoint, epoch))\n",
    "        if valid_auc <= best_valid_auc:\n",
    "            iteration += 1\n",
    "            print('AUC was not improved, iteration {0}'.format(str(iteration)))\n",
    "        else:\n",
    "            print('Saving state')\n",
    "            iteration = 0\n",
    "            best_valid_auc = valid_auc\n",
    "            state = {\n",
    "                'valid_auc': valid_auc,\n",
    "                'valid_loss': valid_loss,\n",
    "                'epoch': epoch,\n",
    "            }\n",
    "            if not os.path.isdir(args.checkpoint):\n",
    "                os.mkdir(args.checkpoint)\n",
    "            torch.save(state, './{}/ckpt.pt'.format(args.checkpoint))\n",
    "        epoch += 1\n",
    "        print(f'Elapsed seconds: ({time.time() - t0:.0f}s)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ca3ba0",
   "metadata": {
    "papermill": {
     "duration": 0.02377,
     "end_time": "2023-06-13T22:21:36.892558",
     "exception": false,
     "start_time": "2023-06-13T22:21:36.868788",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d0b1e17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-13T22:21:36.943608Z",
     "iopub.status.busy": "2023-06-13T22:21:36.942680Z",
     "iopub.status.idle": "2023-06-13T22:21:50.546932Z",
     "shell.execute_reply": "2023-06-13T22:21:50.545732Z"
    },
    "papermill": {
     "duration": 13.632741,
     "end_time": "2023-06-13T22:21:50.549492",
     "exception": false,
     "start_time": "2023-06-13T22:21:36.916751",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model (epoch 13)\n",
      "Saving results in submission.csv\n"
     ]
    }
   ],
   "source": [
    "test_dataset = Loader(args.test_path, window_size=args.window_size, window_stride=args.window_stride,\n",
    "    window_type=args.window_type, normalize=args.normalize)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=args.test_batch_size, shuffle=None,\n",
    "    num_workers=args.num_workers, pin_memory=args.cuda, sampler=None)\n",
    "\n",
    "# get best epoch\n",
    "state = torch.load('./{}/ckpt.pt'.format(args.checkpoint))\n",
    "epoch = state['epoch']\n",
    "print(\"Testing model (epoch {})\".format(epoch))\n",
    "model.load_state_dict(torch.load('./{}/model{:03d}.pt'.format(args.checkpoint, epoch)))\n",
    "if args.cuda:\n",
    "    model.cuda()\n",
    "\n",
    "results = 'submission.csv'\n",
    "print(\"Saving results in {}\".format(results))\n",
    "test(test_loader, model, criterion, args.cuda, save=results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15a41e0",
   "metadata": {
    "papermill": {
     "duration": 0.025194,
     "end_time": "2023-06-13T22:21:50.599444",
     "exception": false,
     "start_time": "2023-06-13T22:21:50.574250",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1281.73954,
   "end_time": "2023-06-13T22:21:53.596622",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-06-13T22:00:31.857082",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
